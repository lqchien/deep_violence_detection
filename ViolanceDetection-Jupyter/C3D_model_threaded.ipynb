{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.13.1\n",
    "!pip install FFmpeg\n",
    "!pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _variable_on_cpu(name, shape, initializer, trainable):\n",
    "    with tf.device('/cpu:0'):\n",
    "        var = tf.get_variable(name, shape, initializer=initializer, trainable=trainable)\n",
    "    return var\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, wd, trainable):\n",
    "    var = _variable_on_cpu(name, shape, tf.contrib.layers.xavier_initializer(), trainable=trainable)\n",
    "    #Check this weight_decay loss!\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_decay_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv3d_layer(tensor_in, w_name, b_name, shape_weight, shape_bias, wd, layer_name='conv', trainable=True):\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = _variable_with_weight_decay(w_name, shape_weight, wd, trainable=trainable)\n",
    "        b = _variable_on_cpu(b_name, shape_bias, tf.constant_initializer(0.0), trainable=trainable)\n",
    "        tf.add_to_collection(tf.GraphKeys.MODEL_VARIABLES, w)\n",
    "        tf.add_to_collection(tf.GraphKeys.MODEL_VARIABLES, b)\n",
    "        \n",
    "        conv = tf.nn.conv3d(tensor_in, w, strides=[1,1,1,1,1], padding='SAME',name='conv')\n",
    "        conv_b = tf.nn.bias_add(conv, b, name='conv_bias')\n",
    "        act = tf.nn.relu(conv_b, 'relu')\n",
    "        tf.summary.histogram('Convolution_Weights', w)\n",
    "        tf.summary.histogram('Conv_Biases', b)\n",
    "        tf.summary.histogram('Conv_Activations', act)\n",
    "        return act\n",
    "\n",
    "def fc_layer(tensor_in, w_name, b_name, shape_weight, shape_bias, wd, _dropout, layer_name='fc', trainable=True):\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = _variable_with_weight_decay(w_name, shape_weight, wd, trainable=trainable)\n",
    "        b = _variable_on_cpu(b_name, shape_bias, tf.constant_initializer(0.0), trainable=trainable)\n",
    "        #tf.add_to_collection(tf.GraphKeys.MODEL_VARIABLES, w)\n",
    "        #tf.add_to_collection(tf.GraphKeys.MODEL_VARIABLES, b)\n",
    "        \n",
    "        tf.add_to_collection('fc_layer', w)\n",
    "        tf.add_to_collection('fc_layer', b)\n",
    "        \n",
    "        fc_1 = tf.matmul(tensor_in, w, name='fc_linear')\n",
    "        fc_b = tf.nn.bias_add(fc_1, b, name='fc_bias')\n",
    "        act = tf.nn.relu(fc_b, 'relu')\n",
    "        tensor_out = tf.nn.dropout(act, _dropout, name='dropout')\n",
    "        tf.summary.histogram('FC_Weights', w)\n",
    "        tf.summary.histogram('FC_Biases', b)\n",
    "        tf.summary.histogram('FC_Activations', act)\n",
    "        return tensor_out\n",
    "        \n",
    "def max_pool_3d(tensor_in, k, layer_name='max_pool'):\n",
    "    with tf.variable_scope(layer_name):\n",
    "        k_size=[1,k,2,2,1]\n",
    "        tensor_out = tf.nn.max_pool3d(tensor_in, ksize=k_size, strides=k_size, padding='SAME')\n",
    "        return tensor_out\n",
    "    \n",
    "def c3d_model(_X, model_settings):\n",
    "    \n",
    "    wd = model_settings['weight_decay']\n",
    "    #Convolution 1a\n",
    "    conv1 = conv3d_layer(_X, 'wc1a', 'bc1a', [3, 3, 3, 3, 64], [64], None, 'Convolution_1a', False)\n",
    "    pool1 = max_pool_3d(conv1, 1, 'Max_Pooling_1')\n",
    "    \n",
    "    #Convolution 2a\n",
    "    conv2 = conv3d_layer(pool1, 'wc2a', 'bc2a', [3, 3, 3, 64, 128], [128], None, 'Convolution_2a', False)\n",
    "    pool2 = max_pool_3d(conv2, 2, 'Max_Pooling_2')\n",
    "    \n",
    "    #Convolution 3a, 3b\n",
    "    conv3a = conv3d_layer(pool2, 'wc3a', 'bc3a',  [3, 3, 3, 128, 256], [256], None, 'Convolution_3a', False)\n",
    "    conv3b = conv3d_layer(conv3a, 'wc3b', 'bc3b', [3, 3, 3, 256, 256], [256], None, 'Convolution_3b', False)\n",
    "    pool3 = max_pool_3d(conv3b, 2, 'Max_Pooling_3')\n",
    "    \n",
    "    #Convolution 4a, 4b\n",
    "    conv4a = conv3d_layer(pool3, 'wc4a', 'bc4a',  [3, 3, 3, 256, 512], [512], None, 'Convolution_4a', False)\n",
    "    conv4b = conv3d_layer(conv4a, 'wc4b', 'bc4b', [3, 3, 3, 512, 512], [512], None, 'Convolution_4b', False)\n",
    "    pool4 = max_pool_3d(conv4b, 2, 'Max_Pooling_4')\n",
    "    \n",
    "    #Convolution 4a, 4b\n",
    "    conv5a = conv3d_layer(pool4, 'wc5a', 'bc5a',  [3, 3, 3, 512, 512], [512], None, 'Convolution_5a', False)\n",
    "    conv5b = conv3d_layer(conv5a, 'wc5b', 'bc5b', [3, 3, 3, 512, 512], [512], None, 'Convolution_5b', False)\n",
    "    pool5 = max_pool_3d(conv5b, 2, 'Max_Pooling_5')\n",
    "\n",
    "    #Reshape weights for fc6\n",
    "    pool5 = tf.transpose(pool5, perm=[0,1,4,2,3])\n",
    "    dense1 = tf.reshape(pool5, [model_settings['batch_size'], 8192])\n",
    "\n",
    "    #FC6 and FC7\n",
    "    dense1 = fc_layer(dense1, 'wd1', 'bd1', [8192, 4096], [4096], wd,\n",
    "                      model_settings['dropout_placeholder'], 'FC6', True)\n",
    "    dense2 = fc_layer(dense1, 'wd2', 'bd2', [4096, 4096], [4096], wd,\n",
    "                      model_settings['dropout_placeholder'], 'FC7', True)\n",
    "    \n",
    "    model_settings['clip_features'] = dense2\n",
    "    \n",
    "    with tf.variable_scope('Softmax_Linear'):\n",
    "        w_out = _variable_with_weight_decay('wout', [4096, 101], wd, True)\n",
    "        b_out = _variable_on_cpu('bout', [101], tf.constant_initializer(0.0), True)\n",
    "        #tf.add_to_collection(tf.GraphKeys.MODEL_VARIABLES, w_out)\n",
    "        #tf.add_to_collection(tf.GraphKeys.MODEL_VARIABLES, b_out)\n",
    "        out = tf.matmul(dense2, w_out) + b_out\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_tower_loss(loss_var_scope, logit, labels):\n",
    "    with tf.variable_scope(loss_var_scope):\n",
    "        \n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, \n",
    "                                                                       logits=logit,\n",
    "                                                                       name='cross_entropy_per_class')\n",
    "        cross_entropy_mean = tf.reduce_mean(cross_entropy, name='Cross_Entropy_Mean')\n",
    "        \n",
    "        weight_losses = tf.get_collection('losses')\n",
    "        tf.summary.scalar('Weight_decay_loss', tf.reduce_mean(weight_losses)) \n",
    "        \n",
    "        tf.add_to_collection('losses', cross_entropy_mean)\n",
    "        total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "#Updates:\n",
    "##Add top-n correct predictions for ground up training\n",
    "def calc_tower_accuracy(logit, labels):\n",
    "    \n",
    "    correct_predictions = tf.equal(labels, tf.argmax(logit, 1))\n",
    "    total_correct = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "    return total_correct\n",
    "    \n",
    "def average_gradients(tower_grads):\n",
    "    \n",
    "    average_grads=[]\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "            # Average over the 'tower' dimension.\n",
    "            grad = tf.concat(axis=0, values=grads)\n",
    "            grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "            # Keep in mind that the Variables are redundant because they are shared\n",
    "            # across towers. So .. we will just return the first tower's pointer to\n",
    "            # the Variable.\n",
    "            v = grad_and_vars[0][1]\n",
    "            grad_and_var = (grad, v)\n",
    "            average_grads.append(grad_and_var)\n",
    "    return average_grads\n",
    "        \n",
    "def show_running_info(model_settings, duration, step, loss, accuracy):\n",
    "    batch_size, num_gpu = model_settings['batch_size'], model_settings['num_gpu']\n",
    "    num_examples_per_step = model_settings['total_batch']\n",
    "    examples_per_sec = num_examples_per_step / duration\n",
    "    sec_per_batch = duration / num_gpu\n",
    "    format_str = ('%s: step %d, (%.1f examples/sec; %.3f'\n",
    "                  'sec/batch), (accuracy: %f loss: %f)')\n",
    "    format_tuple = (datetime.now(), step,\n",
    "                    examples_per_sec, sec_per_batch, accuracy, loss)\n",
    "    print (format_str % format_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_UCF101_dir(filename):\n",
    "    dir_images = []\n",
    "    start_clip = []\n",
    "    label_images = []\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in reader:\n",
    "            dir_images.append(row[0])\n",
    "            start_clip.append(int(row[1]))\n",
    "            label_images.append(row[2])\n",
    "    return dir_images, start_clip, label_images\n",
    "\n",
    "def shuffle_list(dir_clips, start_clips, label_clips, seed=time.time()):\n",
    "    video_indices = list(range(len(dir_clips)))\n",
    "    random.shuffle(video_indices)\n",
    "    shuffled_clip_dirs   = [dir_clips[i] for i in video_indices]\n",
    "    shuffled_starts = [start_clips[i] for i in video_indices]\n",
    "    shuffled_labels = [label_clips[i] for i in video_indices]\n",
    "    return shuffled_clip_dirs, shuffled_starts, shuffled_labels\n",
    "\n",
    "def get_frames_data(filename, start_index, num_frames_per_clip=16):\n",
    "    ret_arr = []\n",
    "    for parent, dirnames, filenames in os.walk(filename):\n",
    "        if(len(filenames) < num_frames_per_clip):\n",
    "            return np.array([])\n",
    "        filenames = sorted(filenames)\n",
    "        for i in range(start_index - 1, start_index + num_frames_per_clip - 1):\n",
    "            image_name = str(filename) + '/' + str(filenames[i])\n",
    "            img = Image.open(image_name)\n",
    "            img_data = np.array(img)\n",
    "            ret_arr.append(img_data)\n",
    "    return ret_arr\n",
    "\n",
    "def read_clip(dirname, start_index, model_settings):\n",
    "    num_frames_per_clip = model_settings['frames_per_clip']\n",
    "    crop_size = model_settings['crop_size']\n",
    "    np_mean = model_settings['np_mean']\n",
    "    tmp_data = get_frames_data(dirname, start_index)\n",
    "    if(len(tmp_data) == 0):\n",
    "        return np.array([])\n",
    "    img_datas=[]\n",
    "    horizontal_flip = random.random()\n",
    "    for j in range(len(tmp_data)):\n",
    "        img = Image.fromarray(tmp_data[j].astype(np.uint8))\n",
    "        if(img.width>img.height):\n",
    "            scale = float(crop_size)/float(img.height)\n",
    "            img = np.array(cv2.resize(np.array(img),(int(img.width * scale + 1), crop_size))).astype(np.float32)\n",
    "        else:\n",
    "            scale = float(crop_size)/float(img.width)\n",
    "            img = np.array(cv2.resize(np.array(img),(crop_size, int(img.height * scale + 1)))).astype(np.float32)\n",
    "        crop_x = int((img.shape[0] - crop_size)/2)\n",
    "        crop_y = int((img.shape[1] - crop_size)/2)\n",
    "        img = img[crop_x:crop_x+crop_size, crop_y:crop_y+crop_size,:] - np_mean[j]\n",
    "        \n",
    "        #Flip the image 0.5 chance\n",
    "        if(horizontal_flip > 0.5):\n",
    "            img = np.fliplr(img)\n",
    "            \n",
    "        img_datas.append(img)\n",
    "    return np.array(img_datas).astype(np.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_placeholders(model_settings):\n",
    "    total_batch = model_settings['total_batch']\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=model_settings['input_shape'], name=\"input_clip\")\n",
    "    labels_placeholder = tf.placeholder(tf.int64, shape=(), name=\"labels\")\n",
    "    dropout_placeholder = tf.placeholder_with_default(model_settings['dropout'], shape=())\n",
    "    #tf.summary.image('input_batch_sample_image', images_placeholder[:,0], total_batch)\n",
    "    \n",
    "    model_settings['images_placeholder'] = images_placeholder\n",
    "    model_settings['labels_placeholder'] = labels_placeholder\n",
    "    model_settings['dropout_placeholder'] = dropout_placeholder\n",
    "    return\n",
    "\n",
    "def set_queue(model_settings):\n",
    "    images_placeholder = model_settings['images_placeholder']\n",
    "    labels_placeholder = model_settings['labels_placeholder']\n",
    "    \n",
    "    queue = tf.FIFOQueue(model_settings['queue_size'], \n",
    "                         [tf.float32, tf.int64], \n",
    "                         shapes=[model_settings['input_shape'],\n",
    "                                 labels_placeholder.shape],\n",
    "                        name='Input_Queue')\n",
    "\n",
    "    enqueue_op = queue.enqueue([model_settings['images_placeholder'], \n",
    "                                model_settings['labels_placeholder']], \n",
    "                               name='Enqueue_Operation')\n",
    "    \n",
    "    model_settings['queue'], model_settings['enqueue_op'] = queue, enqueue_op\n",
    "    \n",
    "#Read single clip at a time \n",
    "def load_and_enqueue(sess, model_settings, thread_index):\n",
    "    coord, enqueue_op = model_settings['coord'], model_settings['enqueue_op']\n",
    "    dir_clips, start_clips, label_clips = model_settings['train_list']\n",
    "    num_thread = model_settings['num_thread']\n",
    "    images_placeholder = model_settings['images_placeholder']\n",
    "    labels_placeholder = model_settings['labels_placeholder']\n",
    "    data_size = len(dir_clips)\n",
    "    \n",
    "    read_index = thread_index\n",
    "    while not coord.should_stop():        \n",
    "        clip_dir, start_index, label = dir_clips[read_index], start_clips[read_index], label_clips[read_index]\n",
    "        clip_dir =  model_settings['data_home'] + clip_dir\n",
    "        input_clip = read_clip(clip_dir, start_index, model_settings)\n",
    "        \n",
    "        if(input_clip.shape == model_settings['input_shape']):\n",
    "            sess.run(enqueue_op, feed_dict={images_placeholder:input_clip,\n",
    "                                            labels_placeholder:label})\n",
    "        read_index = (read_index + num_thread) % data_size\n",
    "    #print('Stop_requested: %d' % thread_index)\n",
    "    \n",
    "def start_queue_threads(sess, model_settings):\n",
    "    t = []\n",
    "    for i in range(model_settings['num_thread']):\n",
    "        t.append(threading.Thread(target=load_and_enqueue, args=(sess, model_settings, i)))\n",
    "        t[i].start()\n",
    "    return t\n",
    "\n",
    "def stop_threads(sess, threads, model_settings):\n",
    "    coord = model_settings['coord']\n",
    "    model_settings['coord'].request_stop()\n",
    "    queue = model_settings['queue']\n",
    "    for i in range(model_settings['num_thread']):\n",
    "        if(sess.run(queue.size()) < model_settings['total_batch']):\n",
    "            break\n",
    "        sess.run(queue.dequeue_many(model_settings['batch_size']))\n",
    "    model_settings['coord'].join(threads)\n",
    "    print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_forward_graph(model_settings):\n",
    "    \n",
    "    queue = model_settings['queue']\n",
    "    tower_accuracy = []\n",
    "    tower_loss = []\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "        for gpu_index in range(model_settings['num_gpu']):\n",
    "            with tf.device('/gpu:%d' % gpu_index):\n",
    "                with tf.name_scope('Tower_%d' % gpu_index) as scope:\n",
    "                    ind_begin, ind_end= (gpu_index * model_settings['batch_size'],\n",
    "                                         (gpu_index+1) * model_settings['batch_size'])\n",
    "                    \n",
    "                    feed_input, feed_label = queue.dequeue_many(model_settings['batch_size'])\n",
    "                    model_out = c3d_model(feed_input, model_settings)\n",
    "\n",
    "                    loss = calc_tower_loss(scope, model_out, feed_label)\n",
    "                    accuracy = calc_tower_accuracy(model_out, feed_label)\n",
    "                    tower_loss.append(loss)\n",
    "                    tower_accuracy.append(accuracy)\n",
    "\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                    \n",
    "    tower_mean_loss = tf.reduce_mean(tower_loss)\n",
    "    tower_mean_accuracy = tf.reduce_mean(tower_accuracy)\n",
    "    tf.summary.scalar('Total_Loss', tower_mean_loss)\n",
    "    tf.summary.scalar('Top1_Correct_Predictions', tower_mean_accuracy)\n",
    "    \n",
    "    model_settings['tower_mean_loss'] = tower_mean_loss\n",
    "    model_settings['tower_mean_accuracy'] = tower_mean_accuracy\n",
    "    model_settings['tower_loss'] = tower_loss\n",
    "                    \n",
    "def create_backward_graph(model_settings):\n",
    "    \n",
    "    global_step=tf.get_variable('Global_Step',[],\n",
    "                                initializer=tf.constant_initializer(0),\n",
    "                                trainable=False)\n",
    "    \n",
    "    opt, tower_loss = model_settings['optimizer'], model_settings['tower_loss']\n",
    "    \n",
    "    tower_grads=[]\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "        for gpu_index in range(model_settings['num_gpu']):\n",
    "            with tf.device('/gpu:%d' % gpu_index):\n",
    "                with tf.name_scope('Tower_%d' % gpu_index) as scope:\n",
    "                    loss = tower_loss[gpu_index]\n",
    "                    grads = opt.compute_gradients(loss)\n",
    "                    tower_grads.append(grads)\n",
    "    grads = average_gradients(tower_grads)\n",
    "    \n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(model_settings['moving_decay'], global_step)\n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    train_op = tf.group(apply_gradient_op, variable_averages_op)\n",
    "\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    model_settings['train_op'], model_settings['summary_op'] = train_op, summary_op\n",
    "\n",
    "\n",
    "def run_training(sess, model_settings):\n",
    "    \n",
    "    model_settings['optimizer'] = tf.train.AdamOptimizer(model_settings['learning_rate'])\n",
    "\n",
    "    #Set placeholders, queue operations and thread coordinator\n",
    "    set_placeholders(model_settings)\n",
    "    set_queue(model_settings)\n",
    "    \n",
    "    #Create Graph\n",
    "    create_forward_graph(model_settings)\n",
    "    create_backward_graph(model_settings)\n",
    "    \n",
    "    if(model_settings['fc_pretrained'] == True):\n",
    "        for fc_variable in tf.get_collection('fc_layer'):\n",
    "            tf.add_to_collection(tf.GraphKeys.MODEL_VARIABLES, fc_variable)\n",
    "    \n",
    "    #Read training file locations and shuffle\n",
    "    dir_clips, start_clips, label_clips = get_UCF101_dir(model_settings['train_file_loc'])\n",
    "    model_settings['train_list'] = shuffle_list(dir_clips, start_clips, label_clips)\n",
    "\n",
    "    #Initialize file thread Coordinator and Start input reading threads\n",
    "    model_settings['coord'] = tf.train.Coordinator()\n",
    "    read_threads = start_queue_threads(sess, model_settings)\n",
    "\n",
    "    #Save Only Model Variables\n",
    "    saver = tf.train.Saver(tf.model_variables())\n",
    "    model_settings['saver'] = saver\n",
    "    \n",
    "    #Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    #Restore saved model variables\n",
    "    if(model_settings['read_pretrained_model'] == True):\n",
    "        saver.restore(sess, model_settings['model_read_dir'])\n",
    "        \n",
    "    #Tensorboard summary writers\n",
    "    summary_writer = tf.summary.FileWriter(model_settings['checkpoint_dir'])\n",
    "    summary_writer.add_graph(sess.graph)\n",
    "    model_settings['summary_writer'] = summary_writer\n",
    "    \n",
    "    train_op, summary_op = model_settings['train_op'], model_settings['summary_op']\n",
    "    \n",
    "    #Single tower loss implementation for now\n",
    "    loss_op, acc_op = model_settings['tower_mean_loss'], model_settings['tower_mean_accuracy']\n",
    "    \n",
    "    #time.sleep(15)\n",
    "    \n",
    "    print('Training begins:')\n",
    "    for step in range(model_settings['max_steps']):\n",
    "        start_time = time.time()        \n",
    "        #print(sess.run(model_settings['queue'].size()))\n",
    "        \n",
    "        if (step+1) % model_settings['checkpoints'] != 0:\n",
    "            _, tower_mean_loss, tower_mean_acc = sess.run([train_op, loss_op, acc_op])\n",
    "        else:\n",
    "            _, summary_str, tower_mean_loss, tower_mean_acc = sess.run([train_op, summary_op, loss_op, acc_op])\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "        \n",
    "        show_running_info(model_settings, time.time() - start_time,\n",
    "                          step, tower_mean_loss, tower_mean_acc)\n",
    "    stop_threads(sess, read_threads, model_settings)\n",
    "\n",
    "def run_testing(sess, model_settings):\n",
    "    \n",
    "    #Convert to multi-tower setting\n",
    "    print('Testing begins:')\n",
    "    loss_op, acc_op = model_settings['tower_loss'][0], model_settings['tower_accuracy'][0]\n",
    "    np_mean = np.load('crop_mean.npy').reshape([model_settings['frames_per_clip'],\n",
    "                                                model_settings['crop_size'],\n",
    "                                                model_settings['crop_size'],\n",
    "                                                model_settings['channels']])\n",
    "    \n",
    "    data_index = 0\n",
    "    epoch_finished = False\n",
    "    batch_accuracy = []\n",
    "    batch_loss = []\n",
    "    while(not epoch_finished):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        test_images, test_labels, data_index, epoch_finished = read_clip_label_sequentially(\n",
    "            dir_clips, label_clips, np_mean, data_index,\n",
    "            batch_size=model_settings['batch_size'] * model_settings['num_gpu'],\n",
    "            num_frames_per_clip=model_settings['frames_per_clip'],\n",
    "            crop_size= model_settings['crop_size'])\n",
    "        \n",
    "        img_process_duration = time.time() - start_time\n",
    "        \n",
    "        feed_dict = {model_settings['images_placeholder'] : test_images,\n",
    "                    model_settings['labels_placeholder'] : test_labels,\n",
    "                    model_settings['dropout_placeholder'] : 1.0}\n",
    "        \n",
    "        accuracy, loss = sess.run([acc_op, loss_op], feed_dict=feed_dict)\n",
    "        \n",
    "        batch_accuracy.append(accuracy)\n",
    "        batch_loss.append(loss)\n",
    "        print('Data index :%d' % data_index)\n",
    "    \n",
    "    test_accuracy = np.mean(np.array(batch_accuracy))\n",
    "    test_loss = np.mean(np.array(batch_loss))\n",
    "    print('Test accuracy %f, test loss %f!' % (test_accuracy, test_loss))\n",
    "    \n",
    "def calculate_features(sess, model_settings):\n",
    "#Convert to multi-tower setting\n",
    "    feature_extract_op = model_settings['clip_features']\n",
    "    np_mean = np.load('crop_mean.npy').reshape([model_settings['frames_per_clip'],\n",
    "                                                model_settings['crop_size'],\n",
    "                                                model_settings['crop_size'],\n",
    "                                                model_settings['channels']])\n",
    "    \n",
    "    dir_clips, label_clips = get_UCF101_dir(model_settings['feature_file_loc'])\n",
    "    dir_clips, label_clips = shuffle_list(dir_clips, label_clips)\n",
    "    \n",
    "    data_index = 0\n",
    "    epoch_finished = False\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    data_scanned = 0\n",
    "    while(not epoch_finished):\n",
    "        #start_time = time.time()\n",
    "        \n",
    "        test_images, test_labels, data_index, epoch_finished = read_clip_label_sequentially(\n",
    "            dir_clips, label_clips, np_mean, data_index,\n",
    "            batch_size=model_settings['batch_size'] * model_settings['num_gpu'],\n",
    "            num_frames_per_clip=model_settings['frames_per_clip'],\n",
    "            crop_size= model_settings['crop_size'])\n",
    "        data_scanned = data_scanned + model_settings['batch_size']\n",
    "        #img_process_duration = time.time() - start_time\n",
    "        \n",
    "        feed_dict = {model_settings['images_placeholder'] : test_images,\n",
    "                    model_settings['labels_placeholder'] : test_labels,\n",
    "                    model_settings['dropout_placeholder'] : 1.0}\n",
    "        \n",
    "        x_data.append(sess.run(feature_extract_op, feed_dict=feed_dict))\n",
    "        y_data.append(test_labels)\n",
    "        \n",
    "        print('Data index :%d' % data_index)\n",
    "    X_data = np.array(x_data)\n",
    "    Y_data = np.array(y_data)\n",
    "    X_data = X_data.reshape([data_scanned, 4096])\n",
    "    Y_data = Y_data.reshape([data_scanned])\n",
    "    return X_data, Y_data\n",
    "    \n",
    "def save_graph(sess, model_settings):\n",
    "    model_settings['saver'].save(sess, \n",
    "                             model_settings['model_save_dir'],\n",
    "                             write_meta_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings = {'max_steps' : 10000, 'batch_size' : 25, 'frames_per_clip' : 16, \n",
    "                  'crop_size' : 112, 'num_gpu' : 1, 'channels' : 3,\n",
    "                  'checkpoint_dir' : './checkpoints/allrand_fc_train',\n",
    "                  'model_read_dir' : './models/s1m_mod.model',\n",
    "                  'model_save_dir' : './models/C3D_1.model',\n",
    "                  'moving_decay' : 0.9999, 'weight_decay' : 0.00005, 'dropout' : 0.5,\n",
    "                  'learning_rate' : 1e-4, 'checkpoints' : 100,\n",
    "                  'train_file_loc' : './data/dir_files/train_ucf.csv',\n",
    "                  'test_file_loc' : './data/dir_files/test_ucf.csv', \n",
    "                  'data_home' : './data/',\n",
    "                  'feature_file_loc' : './data/dir_files/test_ucf_full.lst',\n",
    "                  'num_thread' : 8, 'queue_size' : 1500,\n",
    "                  'read_pretrained_model' : True}\n",
    "\n",
    "model_settings['total_batch'] = model_settings['batch_size'] * model_settings['num_gpu']\n",
    "model_settings['input_shape'] = (model_settings['frames_per_clip'],\n",
    "                                 model_settings['crop_size'],\n",
    "                                 model_settings['crop_size'],\n",
    "                                 model_settings['channels'])\n",
    "model_settings['np_mean'] = np.load('crop_mean.npy').reshape(model_settings['input_shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/s1m_mod.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/s1m_mod.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begins:\n",
      "2018-01-22 18:20:21.247922: step 0, (8.8 examples/sec; 2.845sec/batch), (accuracy: 0.040000 loss: 5.854966)\n",
      "2018-01-22 18:20:21.761287: step 1, (48.8 examples/sec; 0.512sec/batch), (accuracy: 0.080000 loss: 5.879155)\n",
      "2018-01-22 18:20:22.259613: step 2, (50.4 examples/sec; 0.496sec/batch), (accuracy: 0.000000 loss: 5.900627)\n",
      "2018-01-22 18:20:22.757937: step 3, (50.6 examples/sec; 0.494sec/batch), (accuracy: 0.000000 loss: 5.782173)\n",
      "2018-01-22 18:20:23.271303: step 4, (48.8 examples/sec; 0.512sec/batch), (accuracy: 0.000000 loss: 5.284870)\n",
      "2018-01-22 18:20:23.788679: step 5, (48.7 examples/sec; 0.513sec/batch), (accuracy: 0.040000 loss: 5.010622)\n",
      "2018-01-22 18:20:24.293019: step 6, (49.8 examples/sec; 0.502sec/batch), (accuracy: 0.000000 loss: 5.023072)\n",
      "2018-01-22 18:20:24.788338: step 7, (50.6 examples/sec; 0.494sec/batch), (accuracy: 0.040000 loss: 4.594900)\n",
      "2018-01-22 18:20:25.277637: step 8, (51.2 examples/sec; 0.488sec/batch), (accuracy: 0.160000 loss: 4.402936)\n",
      "2018-01-22 18:20:25.770950: step 9, (50.8 examples/sec; 0.492sec/batch), (accuracy: 0.040000 loss: 4.288034)\n",
      "2018-01-22 18:20:26.279301: step 10, (49.4 examples/sec; 0.506sec/batch), (accuracy: 0.000000 loss: 4.735283)\n",
      "2018-01-22 18:20:26.783644: step 11, (49.8 examples/sec; 0.502sec/batch), (accuracy: 0.120000 loss: 4.366247)\n",
      "2018-01-22 18:20:27.280966: step 12, (50.4 examples/sec; 0.496sec/batch), (accuracy: 0.040000 loss: 4.653460)\n",
      "2018-01-22 18:20:27.776282: step 13, (50.6 examples/sec; 0.494sec/batch), (accuracy: 0.000000 loss: 4.585577)\n",
      "2018-01-22 18:20:28.278618: step 14, (50.0 examples/sec; 0.500sec/batch), (accuracy: 0.040000 loss: 4.536561)\n",
      "2018-01-22 18:20:28.773936: step 15, (50.7 examples/sec; 0.493sec/batch), (accuracy: 0.040000 loss: 4.241490)\n",
      "2018-01-22 18:20:29.274266: step 16, (50.1 examples/sec; 0.499sec/batch), (accuracy: 0.040000 loss: 4.441385)\n",
      "2018-01-22 18:20:29.778606: step 17, (49.7 examples/sec; 0.503sec/batch), (accuracy: 0.200000 loss: 4.131610)\n",
      "2018-01-22 18:20:30.270915: step 18, (51.0 examples/sec; 0.490sec/batch), (accuracy: 0.080000 loss: 4.196038)\n",
      "2018-01-22 18:20:30.780270: step 19, (49.2 examples/sec; 0.508sec/batch), (accuracy: 0.080000 loss: 4.307045)\n",
      "2018-01-22 18:20:31.282607: step 20, (49.9 examples/sec; 0.501sec/batch), (accuracy: 0.080000 loss: 4.402622)\n",
      "2018-01-22 18:20:31.789956: step 21, (49.5 examples/sec; 0.505sec/batch), (accuracy: 0.200000 loss: 4.145821)\n",
      "2018-01-22 18:20:32.302318: step 22, (48.8 examples/sec; 0.512sec/batch), (accuracy: 0.080000 loss: 4.534478)\n",
      "2018-01-22 18:20:32.807662: step 23, (49.7 examples/sec; 0.503sec/batch), (accuracy: 0.120000 loss: 4.083343)\n",
      "2018-01-22 18:20:33.312004: step 24, (49.6 examples/sec; 0.504sec/batch), (accuracy: 0.080000 loss: 4.395350)\n",
      "2018-01-22 18:20:33.798297: step 25, (51.5 examples/sec; 0.485sec/batch), (accuracy: 0.200000 loss: 3.916211)\n",
      "2018-01-22 18:20:34.302637: step 26, (49.8 examples/sec; 0.502sec/batch), (accuracy: 0.080000 loss: 4.499724)\n",
      "2018-01-22 18:20:34.806978: step 27, (49.8 examples/sec; 0.502sec/batch), (accuracy: 0.280000 loss: 3.580127)\n",
      "2018-01-22 18:20:35.306468: step 28, (50.3 examples/sec; 0.497sec/batch), (accuracy: 0.040000 loss: 4.063234)\n",
      "2018-01-22 18:20:35.821839: step 29, (49.0 examples/sec; 0.510sec/batch), (accuracy: 0.200000 loss: 3.677986)\n",
      "2018-01-22 18:20:36.321167: step 30, (50.2 examples/sec; 0.498sec/batch), (accuracy: 0.240000 loss: 3.670891)\n",
      "2018-01-22 18:20:36.812473: step 31, (51.0 examples/sec; 0.490sec/batch), (accuracy: 0.240000 loss: 3.646434)\n",
      "2018-01-22 18:20:37.295758: step 32, (51.9 examples/sec; 0.481sec/batch), (accuracy: 0.200000 loss: 3.930578)\n",
      "2018-01-22 18:20:37.808121: step 33, (48.9 examples/sec; 0.511sec/batch), (accuracy: 0.120000 loss: 3.997811)\n",
      "2018-01-22 18:20:38.303437: step 34, (50.7 examples/sec; 0.493sec/batch), (accuracy: 0.080000 loss: 3.722534)\n",
      "2018-01-22 18:20:38.803768: step 35, (50.1 examples/sec; 0.499sec/batch), (accuracy: 0.320000 loss: 3.400117)\n",
      "2018-01-22 18:20:39.301090: step 36, (50.3 examples/sec; 0.497sec/batch), (accuracy: 0.280000 loss: 3.719013)\n",
      "2018-01-22 18:20:39.814456: step 37, (48.8 examples/sec; 0.512sec/batch), (accuracy: 0.240000 loss: 3.803196)\n",
      "2018-01-22 18:20:40.324812: step 38, (49.2 examples/sec; 0.508sec/batch), (accuracy: 0.280000 loss: 3.288739)\n",
      "2018-01-22 18:20:40.821132: step 39, (50.5 examples/sec; 0.495sec/batch), (accuracy: 0.080000 loss: 4.211987)\n",
      "2018-01-22 18:20:41.317452: step 40, (50.4 examples/sec; 0.496sec/batch), (accuracy: 0.160000 loss: 3.544802)\n",
      "2018-01-22 18:20:41.823799: step 41, (49.4 examples/sec; 0.506sec/batch), (accuracy: 0.280000 loss: 3.637800)\n",
      "2018-01-22 18:20:42.326860: step 42, (49.9 examples/sec; 0.501sec/batch), (accuracy: 0.360000 loss: 3.571817)\n",
      "2018-01-22 18:20:42.831201: step 43, (49.7 examples/sec; 0.503sec/batch), (accuracy: 0.280000 loss: 3.395959)\n",
      "2018-01-22 18:20:43.332533: step 44, (49.9 examples/sec; 0.501sec/batch), (accuracy: 0.160000 loss: 4.092310)\n",
      "2018-01-22 18:20:43.837878: step 45, (49.5 examples/sec; 0.505sec/batch), (accuracy: 0.200000 loss: 3.421674)\n",
      "2018-01-22 18:20:44.338409: step 46, (50.0 examples/sec; 0.500sec/batch), (accuracy: 0.440000 loss: 2.644133)\n",
      "2018-01-22 18:20:44.851718: step 47, (48.8 examples/sec; 0.512sec/batch), (accuracy: 0.360000 loss: 3.083225)\n",
      "2018-01-22 18:20:45.359067: step 48, (49.4 examples/sec; 0.506sec/batch), (accuracy: 0.360000 loss: 3.276283)\n",
      "2018-01-22 18:20:45.855443: step 49, (50.5 examples/sec; 0.495sec/batch), (accuracy: 0.280000 loss: 2.975995)\n",
      "2018-01-22 18:20:46.359784: step 50, (49.7 examples/sec; 0.503sec/batch), (accuracy: 0.280000 loss: 3.166668)\n",
      "2018-01-22 18:20:46.862120: step 51, (49.9 examples/sec; 0.501sec/batch), (accuracy: 0.400000 loss: 3.027463)\n",
      "2018-01-22 18:20:47.373480: step 52, (49.0 examples/sec; 0.510sec/batch), (accuracy: 0.240000 loss: 3.113079)\n",
      "2018-01-22 18:20:47.863783: step 53, (51.1 examples/sec; 0.489sec/batch), (accuracy: 0.160000 loss: 3.669564)\n",
      "2018-01-22 18:20:48.361106: step 54, (50.4 examples/sec; 0.496sec/batch), (accuracy: 0.280000 loss: 2.990970)\n",
      "2018-01-22 18:20:48.848402: step 55, (51.4 examples/sec; 0.486sec/batch), (accuracy: 0.200000 loss: 3.609144)\n",
      "2018-01-22 18:20:49.342718: step 56, (50.9 examples/sec; 0.491sec/batch), (accuracy: 0.360000 loss: 3.033769)\n",
      "2018-01-22 18:20:49.844050: step 57, (50.1 examples/sec; 0.499sec/batch), (accuracy: 0.360000 loss: 3.235318)\n",
      "2018-01-22 18:20:50.347388: step 58, (49.9 examples/sec; 0.501sec/batch), (accuracy: 0.440000 loss: 3.023653)\n",
      "2018-01-22 18:20:50.847718: step 59, (50.2 examples/sec; 0.498sec/batch), (accuracy: 0.200000 loss: 3.370034)\n",
      "2018-01-22 18:20:51.343036: step 60, (50.6 examples/sec; 0.494sec/batch), (accuracy: 0.280000 loss: 3.348949)\n",
      "2018-01-22 18:20:51.847378: step 61, (49.8 examples/sec; 0.502sec/batch), (accuracy: 0.520000 loss: 2.314395)\n",
      "2018-01-22 18:20:52.349713: step 62, (49.9 examples/sec; 0.501sec/batch), (accuracy: 0.360000 loss: 2.833488)\n",
      "2018-01-22 18:20:52.848639: step 63, (50.2 examples/sec; 0.498sec/batch), (accuracy: 0.400000 loss: 2.658120)\n",
      "2018-01-22 18:20:53.352980: step 64, (49.7 examples/sec; 0.503sec/batch), (accuracy: 0.440000 loss: 2.627999)\n",
      "2018-01-22 18:20:53.848297: step 65, (50.7 examples/sec; 0.493sec/batch), (accuracy: 0.320000 loss: 3.168213)\n",
      "2018-01-22 18:20:54.341609: step 66, (50.8 examples/sec; 0.492sec/batch), (accuracy: 0.160000 loss: 3.148774)\n",
      "2018-01-22 18:20:54.819880: step 67, (52.4 examples/sec; 0.477sec/batch), (accuracy: 0.200000 loss: 3.688640)\n",
      "2018-01-22 18:20:55.320211: step 68, (50.1 examples/sec; 0.499sec/batch), (accuracy: 0.320000 loss: 3.088688)\n",
      "2018-01-22 18:20:55.797480: step 69, (52.5 examples/sec; 0.476sec/batch), (accuracy: 0.560000 loss: 2.106799)\n",
      "2018-01-22 18:20:56.299816: step 70, (49.8 examples/sec; 0.502sec/batch), (accuracy: 0.280000 loss: 3.134708)\n",
      "2018-01-22 18:20:56.795133: step 71, (50.6 examples/sec; 0.494sec/batch), (accuracy: 0.400000 loss: 2.482046)\n",
      "2018-01-22 18:20:57.302482: step 72, (49.4 examples/sec; 0.506sec/batch), (accuracy: 0.360000 loss: 2.886973)\n",
      "2018-01-22 18:20:57.811470: step 73, (49.3 examples/sec; 0.507sec/batch), (accuracy: 0.200000 loss: 2.992676)\n",
      "2018-01-22 18:20:58.302777: step 74, (50.9 examples/sec; 0.491sec/batch), (accuracy: 0.400000 loss: 2.554742)\n",
      "2018-01-22 18:20:58.794083: step 75, (51.0 examples/sec; 0.490sec/batch), (accuracy: 0.360000 loss: 3.188313)\n",
      "2018-01-22 18:20:59.287396: step 76, (50.8 examples/sec; 0.492sec/batch), (accuracy: 0.320000 loss: 3.069075)\n",
      "2018-01-22 18:20:59.791736: step 77, (49.9 examples/sec; 0.501sec/batch), (accuracy: 0.440000 loss: 2.321906)\n",
      "2018-01-22 18:21:00.301091: step 78, (49.1 examples/sec; 0.509sec/batch), (accuracy: 0.400000 loss: 2.365983)\n",
      "2018-01-22 18:21:00.791394: step 79, (51.2 examples/sec; 0.488sec/batch), (accuracy: 0.400000 loss: 2.449135)\n",
      "2018-01-22 18:21:01.296739: step 80, (49.6 examples/sec; 0.504sec/batch), (accuracy: 0.480000 loss: 2.168476)\n",
      "2018-01-22 18:21:01.792055: step 81, (50.6 examples/sec; 0.494sec/batch), (accuracy: 0.280000 loss: 3.239702)\n",
      "2018-01-22 18:21:02.299404: step 82, (49.4 examples/sec; 0.506sec/batch), (accuracy: 0.160000 loss: 3.498510)\n",
      "2018-01-22 18:21:02.806754: step 83, (49.5 examples/sec; 0.505sec/batch), (accuracy: 0.400000 loss: 2.811160)\n",
      "2018-01-22 18:21:03.303074: step 84, (50.5 examples/sec; 0.495sec/batch), (accuracy: 0.480000 loss: 2.457589)\n",
      "2018-01-22 18:21:03.803404: step 85, (50.1 examples/sec; 0.499sec/batch), (accuracy: 0.440000 loss: 2.301930)\n",
      "2018-01-22 18:21:04.304738: step 86, (50.0 examples/sec; 0.500sec/batch), (accuracy: 0.440000 loss: 2.052550)\n",
      "2018-01-22 18:21:04.802060: step 87, (50.4 examples/sec; 0.496sec/batch), (accuracy: 0.480000 loss: 2.205825)\n",
      "2018-01-22 18:21:05.296374: step 88, (50.7 examples/sec; 0.493sec/batch), (accuracy: 0.520000 loss: 2.302202)\n",
      "2018-01-22 18:21:05.791691: step 89, (50.9 examples/sec; 0.491sec/batch), (accuracy: 0.360000 loss: 2.279971)\n",
      "2018-01-22 18:21:06.268961: step 90, (52.6 examples/sec; 0.475sec/batch), (accuracy: 0.520000 loss: 2.121562)\n",
      "2018-01-22 18:21:06.777313: step 91, (49.2 examples/sec; 0.508sec/batch), (accuracy: 0.440000 loss: 2.599234)\n",
      "2018-01-22 18:21:07.272630: step 92, (50.5 examples/sec; 0.495sec/batch), (accuracy: 0.480000 loss: 2.064617)\n",
      "2018-01-22 18:21:07.763936: step 93, (51.3 examples/sec; 0.487sec/batch), (accuracy: 0.520000 loss: 1.882895)\n",
      "2018-01-22 18:21:08.264267: step 94, (50.1 examples/sec; 0.499sec/batch), (accuracy: 0.440000 loss: 2.277520)\n",
      "2018-01-22 18:21:08.769610: step 95, (49.6 examples/sec; 0.504sec/batch), (accuracy: 0.520000 loss: 1.879455)\n",
      "2018-01-22 18:21:09.265930: step 96, (50.4 examples/sec; 0.496sec/batch), (accuracy: 0.480000 loss: 2.208296)\n",
      "2018-01-22 18:21:09.767263: step 97, (50.1 examples/sec; 0.499sec/batch), (accuracy: 0.360000 loss: 2.823615)\n",
      "2018-01-22 18:21:10.260575: step 98, (50.7 examples/sec; 0.493sec/batch), (accuracy: 0.520000 loss: 1.945479)\n",
      "2018-01-22 18:21:23.767492: step 99, (1.9 examples/sec; 13.506sec/batch), (accuracy: 0.560000 loss: 1.897965)\n",
      "2018-01-22 18:21:24.294895: step 100, (47.8 examples/sec; 0.523sec/batch), (accuracy: 0.560000 loss: 1.759622)\n",
      "2018-01-22 18:21:24.776175: step 101, (52.1 examples/sec; 0.480sec/batch), (accuracy: 0.320000 loss: 2.566126)\n",
      "2018-01-22 18:21:25.273498: step 102, (50.4 examples/sec; 0.496sec/batch), (accuracy: 0.480000 loss: 1.802744)\n",
      "2018-01-22 18:21:25.777840: step 103, (49.9 examples/sec; 0.501sec/batch), (accuracy: 0.640000 loss: 1.463899)\n",
      "2018-01-22 18:21:26.288196: step 104, (49.1 examples/sec; 0.509sec/batch), (accuracy: 0.440000 loss: 1.994721)\n",
      "2018-01-22 18:21:26.783513: step 105, (50.7 examples/sec; 0.493sec/batch), (accuracy: 0.640000 loss: 1.753049)\n",
      "2018-01-22 18:21:27.281838: step 106, (50.2 examples/sec; 0.498sec/batch), (accuracy: 0.560000 loss: 1.723819)\n",
      "2018-01-22 18:21:27.781166: step 107, (50.3 examples/sec; 0.497sec/batch), (accuracy: 0.400000 loss: 2.140957)\n",
      "2018-01-22 18:21:28.294531: step 108, (49.0 examples/sec; 0.510sec/batch), (accuracy: 0.600000 loss: 1.941864)\n",
      "2018-01-22 18:21:28.798872: step 109, (49.7 examples/sec; 0.503sec/batch), (accuracy: 0.480000 loss: 2.218263)\n",
      "2018-01-22 18:21:29.304216: step 110, (49.6 examples/sec; 0.504sec/batch), (accuracy: 0.520000 loss: 2.153507)\n",
      "2018-01-22 18:21:29.810562: step 111, (49.5 examples/sec; 0.505sec/batch), (accuracy: 0.480000 loss: 1.771547)"
     ]
    }
   ],
   "source": [
    "model_settings['fc_pretrained'] = True\n",
    "tf.reset_default_graph()\n",
    "with tf.device('/cpu:0'):\n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False)) as sess:\n",
    "        run_training(sess, model_settings)\n",
    "        save_graph(sess, model_settings)\n",
    "\n",
    "print('\\n-------------------------------------\\nTraining 2!\\n')\n",
    "model_settings['fc_pretrained'] = False\n",
    "model_settings['checkpoint_dir'] = './checkpoints/lastrand_fc_train'\n",
    "model_settings['model_save_dir'] = './models/C3D_2.model'\n",
    "model_settings['max_steps'] = 15000\n",
    "tf.reset_default_graph()\n",
    "with tf.device('/cpu:0'):\n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False)) as sess:\n",
    "        run_training(sess, model_settings)\n",
    "        save_graph(sess, model_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add last parameters for saving\n",
    "print('trainable_variables:')\n",
    "for i in tf.trainable_variables():\n",
    "    print(i)\n",
    "\n",
    "print('Model_variables before:')\n",
    "for i in tf.model_variables():\n",
    "    print(i)\n",
    "\n",
    "print('Model_var added variables')\n",
    "for i in tf.trainable_variables()[4:6]:\n",
    "    print(i)\n",
    "    tf.add_to_collection(tf.GraphKeys.MODEL_VARIABLES, i)\n",
    "    \n",
    "print('Model_variables after:')\n",
    "for i in tf.model_variables():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save all parameters\n",
    "saver_full = tf.train.Saver(tf.model_variables())\n",
    "model_settings['saver'] = saver_full\n",
    "save_graph(sess, model_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate new data features\n",
    "sess.close()\n",
    "model_settings['feature_file_loc'] = './data/dir_files/data_fight1000.list'\n",
    "tf.reset_default_graph()\n",
    "with tf.device('/cpu:0'):\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False))\n",
    "    create_forward_graph(model_settings)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.model_variables())\n",
    "    saver.restore(sess, './models/UCF_finetuneFC_last.model')\n",
    "    X_data, y_data = calculate_features(sess, model_settings)\n",
    "    X_data.tofile('./data/features/s1m_f1000_x.npy')\n",
    "    y_data.tofile('./data/features/s1m_f1000_y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_data.tofile('./data/features/s1m_ucf10_3_x.npy')\n",
    "y_data.tofile('./data/features/s1m_ucf10_3_y.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
